# PROJECTNAME

## Objective
This entire repository is just for fun.  I want a deep understanding of some of my favorite NLP models.  I want to reverse engineer them.  

I plan on creating the code for the following NLP architectures.  
- Attention mechanism from "Attention is all you need" 
- T5 architecture from "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
- MAMBA architecture from "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"

### Skills Learned
[Bullet Points - Remove this afterwards]

- Understanding (of NLP concepts): Demonstrates deep knowledge of natural language processing by dissecting and reconstructing complex models
- Programming (in Python): Showcases advanced Python skills necessary for implementing intricate NLP models from scratch.
- Problem-Solving: Enhances the ability to navigate and solve the complex challenges inherent in coding NLP architectures.
- Research: Requires the ability to thoroughly understand and apply intricate details from academic papers on NLP models.
- Attention to Detail: Necessitates a meticulous approach to ensure every component of the model functions as intended.
- Critical Thinking: Encourages critical evaluation of model designs and the consideration of potential improvements.

### Tools Used

- Python, Pytorch
- Pyreverse for developing UML

<!--
## Steps
drag & drop screenshots here or use imgur and reference them using imgsrc

Every screenshot should have some text explaining what the screenshot is about.

Example below.

*Ref 1: Network Diagram*
-->